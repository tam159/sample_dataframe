{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d36228bd",
   "metadata": {},
   "source": [
    "# Import and create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8629efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T03:21:06.805776Z",
     "start_time": "2024-08-31T03:21:06.636748Z"
    }
   },
   "source": [
    "import hashlib\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import Row, SparkSession, Window\n",
    "from pyspark.sql.functions import (avg, broadcast, coalesce, col, concat,\n",
    "                                   count, countDistinct, current_timestamp,\n",
    "                                   date_format, dense_rank, lit, lower, md5,\n",
    "                                   rand, round, sum, to_date, udf, upper, when)\n",
    "from pyspark.sql.types import (DateType, DecimalType, DoubleType, IntegerType,\n",
    "                               LongType, ShortType, StringType, StructField,\n",
    "                               StructType, TimestampType)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ee162273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T03:21:08.055120Z",
     "start_time": "2024-08-31T03:21:08.050363Z"
    }
   },
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", False)\n",
    "conf.set(\"spark.sql.broadcastTimeout\", \"2400\")\n",
    "# conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "# conf.set(\"spark.executor.memory\", \"8g\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x108514bd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "4d78bc5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T03:21:14.679909Z",
     "start_time": "2024-08-31T03:21:08.280929Z"
    }
   },
   "source": [
    "spark = (\n",
    "    SparkSession.builder.config(conf=conf)\n",
    "    .appName(\"spark-jobs\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/31 10:21:11 WARN Utils: Your hostname, MayM1.local resolves to a loopback address: 127.0.0.1; using 192.168.1.5 instead (on interface en0)\n",
      "24/08/31 10:21:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/31 10:21:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "5875f5c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T03:21:14.696643Z",
     "start_time": "2024-08-31T03:21:14.681093Z"
    }
   },
   "source": [
    "log4j = spark._jvm.org.apache.log4j\n",
    "logger = log4j.LogManager.getLogger(__name__)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "84f8978e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T03:21:15.355584Z",
     "start_time": "2024-08-31T03:21:15.352055Z"
    }
   },
   "source": "spark.version",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "7396bb00",
   "metadata": {},
   "source": [
    "# Reading sample data"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1cb8761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T03:03:03.726234Z",
     "start_time": "2024-08-31T03:02:57.620521Z"
    }
   },
   "source": [
    "products_df = spark.read.parquet(\"./data/products_parquet\")\n",
    "# sales_df = spark.read.parquet(\"./data/sales_parquet\")\n",
    "# sellers_df = spark.read.parquet(\"./data/sellers_parquet\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "71a7d57d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T03:03:05.215022Z",
     "start_time": "2024-08-31T03:03:05.213362Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "fd2585e2",
   "metadata": {},
   "source": [
    "# Discover the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf9827d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T03:03:08.389291Z",
     "start_time": "2024-08-31T03:03:08.370856Z"
    }
   },
   "source": [
    "products_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ce9cf063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T03:03:11.645709Z",
     "start_time": "2024-08-31T03:03:09.160571Z"
    }
   },
   "source": [
    "products_df.show(5)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+\n",
      "|product_id|product_name|price|\n",
      "+----------+------------+-----+\n",
      "|         0|   product_0|   22|\n",
      "|         1|   product_1|   30|\n",
      "|         2|   product_2|   91|\n",
      "|         3|   product_3|   37|\n",
      "|         4|   product_4|  145|\n",
      "+----------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "eed44b90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T03:03:13.518484Z",
     "start_time": "2024-08-31T03:03:11.648661Z"
    }
   },
   "source": [
    "products_df.count()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d75ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3fd4113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_name: string (nullable = true)\n",
      " |-- daily_target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sellers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9a8ff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+\n",
      "|seller_id|seller_name|daily_target|\n",
      "+---------+-----------+------------+\n",
      "|        0|   seller_0|     2500000|\n",
      "|        1|   seller_1|      257237|\n",
      "|        2|   seller_2|      754188|\n",
      "|        3|   seller_3|      310462|\n",
      "|        4|   seller_4|     1532808|\n",
      "+---------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sellers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65dda3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sellers_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3876ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06066caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- num_pieces_sold: string (nullable = true)\n",
      " |-- bill_raw_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82414a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-10|             26|kyeibuumwlyhuwksx...|\n",
      "|       2|         0|        0|2020-07-08|             13|jfyuoyfkeyqkckwbu...|\n",
      "|       3|         0|        0|2020-07-05|             38|uyjihlzhzcswxcccx...|\n",
      "|       4|         0|        0|2020-07-05|             56|umnxvoqbdzpbwjqmz...|\n",
      "|       5|         0|        0|2020-07-05|             11|zmqexmaawmvdpqhih...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae7a3df4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43msales_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/sample_dataframe/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1193\u001B[0m, in \u001B[0;36mDataFrame.count\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcount\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m   1171\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   1172\u001B[0m \n\u001B[1;32m   1173\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;124;03m    3\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1193\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/.virtualenvs/sample_dataframe/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/.virtualenvs/sample_dataframe/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[0;32m~/.virtualenvs/sample_dataframe/lib/python3.11/site-packages/py4j/clientserver.py:284\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 284\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_new_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/.virtualenvs/sample_dataframe/lib/python3.11/site-packages/py4j/clientserver.py:291\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    288\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 291\u001B[0m     \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect_to_java_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/.virtualenvs/sample_dataframe/lib/python3.11/site-packages/py4j/clientserver.py:438\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[0;32m--> 438\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_address\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_port\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "sales_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77c9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66cd8519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===========================================>             (64 + 8) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|seller_id|     cnt|\n",
      "+---------+--------+\n",
      "|        0|19000000|\n",
      "|        9|  111392|\n",
      "|        3|  111328|\n",
      "|        6|  111318|\n",
      "|        2|  111233|\n",
      "|        4|  111168|\n",
      "|        7|  111040|\n",
      "|        8|  110882|\n",
      "|        5|  110874|\n",
      "|        1|  110805|\n",
      "+---------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_df.groupBy(col(\"seller_id\")) \\\n",
    ".agg(count(\"*\").alias(\"cnt\")) \\\n",
    ".orderBy(col(\"cnt\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21993b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bcfcb84",
   "metadata": {},
   "source": [
    "# Warm up exercies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6124275",
   "metadata": {},
   "source": [
    "##   Output how many products have been actually sold at least once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67e753b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products sold at least once\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(product_id)|\n",
      "+-----------------+\n",
      "|           993429|\n",
      "+-----------------+\n",
      "\n",
      "+--------------------------+\n",
      "|count(DISTINCT product_id)|\n",
      "+--------------------------+\n",
      "|                    993429|\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:===================================================>    (76 + 7) / 83]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of products sold at least once\")\n",
    "sales_df.agg(countDistinct(col(\"product_id\"))).show()\n",
    "sales_df.select(countDistinct(\"product_id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fddd22c",
   "metadata": {},
   "source": [
    "##  Output which is the product that has been sold in more orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6b336da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product present in more orders\n"
     ]
    }
   ],
   "source": [
    "print(\"Product present in more orders\")\n",
    "a_df = sales_df.groupBy(col(\"product_id\")) \\\n",
    ".agg(count(\"*\").alias(\"cnt\")) \\\n",
    ".where(col(\"cnt\") > 1) \\\n",
    ".orderBy(col(\"cnt\").desc()) \\\n",
    ".limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "096c83a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=====================================================>  (79 + 4) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|product_id|     cnt|\n",
      "+----------+--------+\n",
      "|         0|19000000|\n",
      "|  31136332|       3|\n",
      "|  32602520|       3|\n",
      "+----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "a_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01c7170f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(product_id='0', cnt=19000000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_df.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27666de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eb7a3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products in more than 1 order\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6582"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of products in more than 1 order\")\n",
    "sales_df.groupBy(col(\"product_id\")).agg(count(\"*\").alias(\"cnt\")).where(\n",
    "    col(\"cnt\") > 1\n",
    ").orderBy(col(\"cnt\").desc()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e8049",
   "metadata": {},
   "source": [
    "## How many distinct products have been sold in each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "849385ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:======================================================> (81 + 2) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|      date|product_count|\n",
      "+----------+-------------+\n",
      "|2020-07-10|        98973|\n",
      "|2020-07-09|       100501|\n",
      "|2020-07-08|        99662|\n",
      "|2020-07-07|        99756|\n",
      "|2020-07-06|       100765|\n",
      "|2020-07-05|        99796|\n",
      "|2020-07-04|        99791|\n",
      "|2020-07-03|       100017|\n",
      "|2020-07-02|        99807|\n",
      "|2020-07-01|       100337|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_df.groupBy(col(\"date\")).agg(\n",
    "    countDistinct(\"product_id\").alias(\"product_count\")\n",
    ").orderBy(col(\"date\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4fd943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a009d58a",
   "metadata": {},
   "source": [
    "# Exercies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e99d5",
   "metadata": {},
   "source": [
    "## 1. What is the average revenue of the orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88408e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|            1246.1338560822878|\n",
      "+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_df.join(\n",
    "    products_df, sales_df[\"product_id\"] == products_df[\"product_id\"], \"inner\"\n",
    ").agg(avg(products_df[\"price\"] * sales_df[\"num_pieces_sold\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e157610",
   "metadata": {},
   "source": [
    "### A skewed join, where one task takes a long time to execute since the join is skewed on a very small number of keys (in this case, product_id = 0)\n",
    "Let’s fix this issue using a technique known as “key salting”. Using the new “salted” key will un-skew the join:\n",
    "- Duplicate the entries that we have in the dimension table for the most common products, e.g. product_0 will be replicated creating the IDs: product_0–1, product_0–2, product_0–3 and so on.\n",
    "- On the sales table, we are going to replace “product_0” with a random replica (e.g. some of them will be replaced with product_0–1, others with product_0–2, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358b59a",
   "metadata": {},
   "source": [
    "#### Step 1 - Check and select the skewed keys \n",
    "In this case we are retrieving the top 100 keys: these will be the only salted keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e1b787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(product_id='0', count=19000000)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = (\n",
    "    sales_df.groupby(sales_df[\"product_id\"])\n",
    "    .count()\n",
    "    .sort(col(\"count\").desc())\n",
    "    .limit(5)\n",
    "    .collect()\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3579f",
   "metadata": {},
   "source": [
    "#### Step 2 - What we want to do is:\n",
    "1. Duplicate the entries that we have in the dimension table for the most common products, e.g. product_0 will become: product_0-1, product_0-2, product_0-3 and so on\n",
    "2. On the sales table, we are going to replace \"product_0\" with a random duplicate (e.g. some of them will be replaced with product_0-1, others with product_0-2, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9105c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLICATION_FACTOR = 101\n",
    "l = []\n",
    "replicated_products = []\n",
    "for _r in results:\n",
    "    replicated_products.append(_r[\"product_id\"])\n",
    "    for _rep in range(0, REPLICATION_FACTOR):\n",
    "        l.append((_r[\"product_id\"], _rep))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "replicated_df = rdd.map(lambda x: Row(product_id=x[0], replication=int(x[1])))\n",
    "replicated_df = spark.createDataFrame(replicated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5763d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|product_id|replication|\n",
      "+----------+-----------+\n",
      "|         0|          0|\n",
      "|         0|          1|\n",
      "|         0|          2|\n",
      "|         0|          3|\n",
      "|         0|          4|\n",
      "+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replicated_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cf978c",
   "metadata": {},
   "source": [
    "####   Step 3: Generate the salted key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62d5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = products_df.join(\n",
    "    broadcast(replicated_df),\n",
    "    products_df[\"product_id\"] == replicated_df[\"product_id\"],\n",
    "    \"left\",\n",
    ").withColumn(\n",
    "    \"salted_join_key\",\n",
    "    when(replicated_df[\"replication\"].isNull(), products_df[\"product_id\"]).otherwise(\n",
    "        concat(replicated_df[\"product_id\"], lit(\"-\"), replicated_df[\"replication\"])\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50b729a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+----------+-----------+---------------+\n",
      "|product_id|product_name|price|product_id|replication|salted_join_key|\n",
      "+----------+------------+-----+----------+-----------+---------------+\n",
      "|         0|   product_0|   22|         0|        100|          0-100|\n",
      "|         0|   product_0|   22|         0|         99|           0-99|\n",
      "|         0|   product_0|   22|         0|         98|           0-98|\n",
      "|         0|   product_0|   22|         0|         97|           0-97|\n",
      "|         0|   product_0|   22|         0|         96|           0-96|\n",
      "+----------+------------+-----+----------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9714834",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df.withColumn(\n",
    "    \"salted_join_key\",\n",
    "    when(\n",
    "        sales_df[\"product_id\"].isin(replicated_products),\n",
    "        concat(\n",
    "            sales_df[\"product_id\"],\n",
    "            lit(\"-\"),\n",
    "            round(rand() * (REPLICATION_FACTOR - 1), 0).cast(IntegerType()),\n",
    "        ),\n",
    "    ).otherwise(sales_df[\"product_id\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79a3ee0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+---------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|salted_join_key|\n",
      "+--------+----------+---------+----------+---------------+--------------------+---------------+\n",
      "|       1|         0|        0|2020-07-10|             26|kyeibuumwlyhuwksx...|           0-86|\n",
      "|       2|         0|        0|2020-07-08|             13|jfyuoyfkeyqkckwbu...|           0-74|\n",
      "|       3|         0|        0|2020-07-05|             38|uyjihlzhzcswxcccx...|           0-30|\n",
      "|       4|         0|        0|2020-07-05|             56|umnxvoqbdzpbwjqmz...|           0-22|\n",
      "|       5|         0|        0|2020-07-05|             11|zmqexmaawmvdpqhih...|           0-23|\n",
      "+--------+----------+---------+----------+---------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b8f7e7",
   "metadata": {},
   "source": [
    "####  Step 4: Finally let's do the join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "873619fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==================================================>     (17 + 2) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|avg((price * num_pieces_sold))|\n",
      "+------------------------------+\n",
      "|            1246.1338560822878|\n",
      "+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:=====================================================>  (18 + 1) / 19]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_df.join(\n",
    "    products_df, sales_df[\"salted_join_key\"] == products_df[\"salted_join_key\"], \"inner\"\n",
    ").agg(avg(products_df[\"price\"] * sales_df[\"num_pieces_sold\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d339a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "144532c4",
   "metadata": {},
   "source": [
    "## 2. For each seller, what is the average % contribution of an order to the seller's daily quota?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f42ed",
   "metadata": {},
   "source": [
    "### Using broadcast for sellers_df\n",
    "- “Broadcasting” simply means that a copy of the table is sent to every executor, allowing to “localize” the task\n",
    "- When we broadcast a table, we need to be sure that this will not become too-big-to-broadcast in the future, otherwise we’ll start to have Out Of Memory errors later in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b2c2e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=============================================>          (68 + 8) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|seller_id|          avg(ratio)|\n",
      "+---------+--------------------+\n",
      "|        0|2.019885898946922...|\n",
      "|        7|2.595228787788170...|\n",
      "|        3| 1.62888537056594E-4|\n",
      "|        8|9.213030375408861E-5|\n",
      "|        5|4.211073965904022E-5|\n",
      "|        6|4.782147194369122E-5|\n",
      "|        9|3.837913136180238E-5|\n",
      "|        1|1.964233366461014...|\n",
      "|        4|3.296428039825817E-5|\n",
      "|        2|6.690408001060484E-5|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:=======================================================>(82 + 1) / 83]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_df.join(\n",
    "    broadcast(sellers_df), sales_df[\"seller_id\"] == sellers_df[\"seller_id\"], \"inner\"\n",
    ").withColumn(\"ratio\", sales_df[\"num_pieces_sold\"] / sellers_df[\"daily_target\"]).groupBy(\n",
    "    sales_df[\"seller_id\"]\n",
    ").agg(\n",
    "    avg(\"ratio\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447f59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eae67118",
   "metadata": {},
   "source": [
    "## 3. Who are the second most selling and the least selling persons (sellers) for each product? Who are those for product with `product_id = 0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd968ff1",
   "metadata": {},
   "source": [
    "### 3.1. Calcuate the number of pieces sold by each seller for each product. Because in sales_df there are different date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18205c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:================================================>       (72 + 8) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------+\n",
      "|product_id|seller_id|total_num_pieces_sold|\n",
      "+----------+---------+---------------------+\n",
      "|  46681458|        9|                 33.0|\n",
      "|  19609336|        5|                 56.0|\n",
      "|  70025887|        4|                 58.0|\n",
      "|  46825944|        9|                 14.0|\n",
      "|  27408703|        4|                 90.0|\n",
      "|  14623915|        9|                 68.0|\n",
      "|  26750979|        3|                 79.0|\n",
      "|  43530510|        8|                 19.0|\n",
      "|  26670637|        3|                 24.0|\n",
      "|  41883875|        8|                 58.0|\n",
      "|  40929610|        6|                 68.0|\n",
      "|  62842664|        3|                 63.0|\n",
      "|  53319290|        6|                 27.0|\n",
      "|  31356937|        7|                 21.0|\n",
      "|  39749661|        4|                 76.0|\n",
      "|  73912193|        3|                 27.0|\n",
      "|  52715543|        5|                 71.0|\n",
      "|  41020103|        7|                 41.0|\n",
      "|  59834452|        6|                 52.0|\n",
      "|  16663667|        7|                 59.0|\n",
      "+----------+---------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 35:===================================================>    (76 + 7) / 83]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_sales_df = sales_df.groupby(col(\"product_id\"), col(\"seller_id\")).agg(\n",
    "    sum(\"num_pieces_sold\").alias(\"total_num_pieces_sold\")\n",
    ")\n",
    "total_sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329572b",
   "metadata": {},
   "source": [
    "### 3.2. Create the window functions, one will sort ascending the other one descending. Partition by the product_id and sort by the pieces sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a51e6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_desc = Window.partitionBy(col(\"product_id\")).orderBy(\n",
    "    col(\"total_num_pieces_sold\").desc()\n",
    ")\n",
    "window_asc = Window.partitionBy(col(\"product_id\")).orderBy(\n",
    "    col(\"total_num_pieces_sold\").asc()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b98d0",
   "metadata": {},
   "source": [
    "### 3.3. Create a Dense Rank (to avoid holes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c1df50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_df = total_sales_df.withColumn(\n",
    "    \"rank_asc\", dense_rank().over(window_asc)\n",
    ").withColumn(\"rank_desc\", dense_rank().over(window_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec1222b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:=================================================>     (74 + 8) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------+--------+---------+\n",
      "|product_id|seller_id|total_num_pieces_sold|rank_asc|rank_desc|\n",
      "+----------+---------+---------------------+--------+---------+\n",
      "|  10000047|        9|                 29.0|       1|        1|\n",
      "|  10000712|        7|                 59.0|       1|        1|\n",
      "|  10000715|        6|                 51.0|       1|        1|\n",
      "|  10001485|        3|                 12.0|       1|        1|\n",
      "|  10001689|        5|                 40.0|       1|        1|\n",
      "+----------+---------+---------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_sales_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0ffff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:===============================================>       (72 + 8) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------+--------+---------+\n",
      "|product_id|seller_id|total_num_pieces_sold|rank_asc|rank_desc|\n",
      "+----------+---------+---------------------+--------+---------+\n",
      "|  10978356|        7|                 27.0|       1|        3|\n",
      "|  14542470|        5|                  3.0|       1|        3|\n",
      "|  17944574|        8|                 15.0|       1|        3|\n",
      "+----------+---------+---------------------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_sales_df.orderBy(col(\"rank_desc\").desc(), col(\"product_id\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9dbc5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:================================================>      (73 + 8) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|seller_count|\n",
      "+----------+------------+\n",
      "|  20774718|           3|\n",
      "|  72017876|           3|\n",
      "|  14542470|           3|\n",
      "+----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_sales_df.groupBy(col(\"product_id\")).agg(\n",
    "    count(\"seller_id\").alias(\"seller_count\")\n",
    ").where(col(\"seller_count\") > 1).orderBy(col(\"seller_count\").desc()).limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b20a335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------+--------+---------+\n",
      "|product_id|seller_id|total_num_pieces_sold|rank_asc|rank_desc|\n",
      "+----------+---------+---------------------+--------+---------+\n",
      "|  20774718|        1|                 88.0|       3|        1|\n",
      "|  20774718|        3|                 16.0|       2|        2|\n",
      "|  20774718|        9|                  5.0|       1|        3|\n",
      "+----------+---------+---------------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_sales_df.filter(col(\"product_id\") == \"20774718\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca8e81",
   "metadata": {},
   "source": [
    "### 3.4. Get products that only have one row OR the products in which multiple sellers sold the same amount (i.e. all the employees that ever sold the product, sold the same exact amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9fab5f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 134:===============================================>       (72 + 9) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------+--------------------+\n",
      "|single_seller_product_id|single_seller_seller_id|                type|\n",
      "+------------------------+-----------------------+--------------------+\n",
      "|                10000047|                      9|Only seller or mu...|\n",
      "|                10000712|                      7|Only seller or mu...|\n",
      "|                10000715|                      6|Only seller or mu...|\n",
      "|                10001485|                      3|Only seller or mu...|\n",
      "|                10001689|                      5|Only seller or mu...|\n",
      "+------------------------+-----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "single_seller_df = total_sales_df.where(col(\"rank_asc\") == col(\"rank_desc\")).select(\n",
    "    col(\"product_id\").alias(\"single_seller_product_id\"),\n",
    "    col(\"seller_id\").alias(\"single_seller_seller_id\"),\n",
    "    lit(\"Only seller or multiple sellers with the same results\").alias(\"type\"),\n",
    ")\n",
    "single_seller_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d316e95",
   "metadata": {},
   "source": [
    "### 3.5. Get the second top sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cff39e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 140:===================================================>   (77 + 6) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------+-----------------+\n",
      "|second_seller_product_id|second_seller_seller_id|             type|\n",
      "+------------------------+-----------------------+-----------------+\n",
      "|                 1015908|                      8|Second top seller|\n",
      "|                10277686|                      4|Second top seller|\n",
      "|                10282881|                      7|Second top seller|\n",
      "|                10646888|                      8|Second top seller|\n",
      "|                10669968|                      3|Second top seller|\n",
      "+------------------------+-----------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "second_seller_df = total_sales_df.where(col(\"rank_desc\") == 2).select(\n",
    "    col(\"product_id\").alias(\"second_seller_product_id\"),\n",
    "    col(\"seller_id\").alias(\"second_seller_seller_id\"),\n",
    "    lit(\"Second top seller\").alias(\"type\"),\n",
    ")\n",
    "second_seller_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402ebef",
   "metadata": {},
   "source": [
    "### 3.6. Get the least sellers and exclude those rows that are already included in the first piece. We also exclude the \"second top sellers\" that are also \"least sellers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a6d5e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "least_seller_df = (\n",
    "    total_sales_df.where(col(\"rank_asc\") == 1)\n",
    "    .select(col(\"product_id\"), col(\"seller_id\"), lit(\"Least Seller\").alias(\"type\"))\n",
    "    .join(\n",
    "        single_seller_df,\n",
    "        (total_sales_df[\"seller_id\"] == single_seller_df[\"single_seller_seller_id\"])\n",
    "        & (\n",
    "            total_sales_df[\"product_id\"] == single_seller_df[\"single_seller_product_id\"]\n",
    "        ),\n",
    "        \"left_anti\",\n",
    "    )\n",
    "    .join(\n",
    "        second_seller_df,\n",
    "        (total_sales_df[\"seller_id\"] == second_seller_df[\"second_seller_seller_id\"])\n",
    "        & (\n",
    "            total_sales_df[\"product_id\"] == second_seller_df[\"second_seller_product_id\"]\n",
    "        ),\n",
    "        \"left_anti\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "740c57c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+\n",
      "|product_id|seller_id|        type|\n",
      "+----------+---------+------------+\n",
      "|  19986717|        1|Least Seller|\n",
      "|  40496308|        5|Least Seller|\n",
      "|  52606213|        7|Least Seller|\n",
      "|   3534470|        3|Least Seller|\n",
      "|  14542470|        5|Least Seller|\n",
      "|  28592106|        5|Least Seller|\n",
      "|  61475460|        7|Least Seller|\n",
      "|  17944574|        8|Least Seller|\n",
      "|  35669461|        4|Least Seller|\n",
      "|  72017876|        1|Least Seller|\n",
      "|  34681047|        5|Least Seller|\n",
      "|  56011040|        5|Least Seller|\n",
      "|  67723231|        5|Least Seller|\n",
      "|  32602520|        9|Least Seller|\n",
      "|  69790381|        5|Least Seller|\n",
      "|  18182299|        7|Least Seller|\n",
      "|  31136332|        9|Least Seller|\n",
      "|  57735075|        9|Least Seller|\n",
      "|  10978356|        7|Least Seller|\n",
      "|  36269838|        8|Least Seller|\n",
      "+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "least_seller_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b227cb",
   "metadata": {},
   "source": [
    "### 3.7. Union all the pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d1df5bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+\n",
      "|product_id|seller_id|        type|\n",
      "+----------+---------+------------+\n",
      "|  19986717|        1|Least Seller|\n",
      "|  14542470|        5|Least Seller|\n",
      "|  28592106|        5|Least Seller|\n",
      "|  40496308|        5|Least Seller|\n",
      "|  52606213|        7|Least Seller|\n",
      "|  61475460|        7|Least Seller|\n",
      "|  17944574|        8|Least Seller|\n",
      "|  72017876|        1|Least Seller|\n",
      "|   3534470|        3|Least Seller|\n",
      "|  35669461|        4|Least Seller|\n",
      "|  34681047|        5|Least Seller|\n",
      "|  56011040|        5|Least Seller|\n",
      "|  67723231|        5|Least Seller|\n",
      "|  32602520|        9|Least Seller|\n",
      "|  69790381|        5|Least Seller|\n",
      "|  10978356|        7|Least Seller|\n",
      "|  18182299|        7|Least Seller|\n",
      "|  36269838|        8|Least Seller|\n",
      "|  20774718|        9|Least Seller|\n",
      "|  31136332|        9|Least Seller|\n",
      "+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 214:===================>                                     (1 + 2) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_sales_union_df = (\n",
    "    least_seller_df.select(col(\"product_id\"), col(\"seller_id\"), col(\"type\"))\n",
    "    .union(\n",
    "        second_seller_df.select(\n",
    "            col(\"second_seller_product_id\").alias(\"product_id\"),\n",
    "            col(\"second_seller_seller_id\").alias(\"seller_id\"),\n",
    "            col(\"type\"),\n",
    "        )\n",
    "    )\n",
    "    .union(\n",
    "        single_seller_df.select(\n",
    "            col(\"single_seller_product_id\").alias(\"product_id\"),\n",
    "            col(\"single_seller_seller_id\").alias(\"seller_id\"),\n",
    "            col(\"type\"),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "total_sales_union_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfeb0ad",
   "metadata": {},
   "source": [
    "### 3.8. Which are the second top seller and least seller of product 20774718?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ce359f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|product_id|seller_id|                type|\n",
      "+----------+---------+--------------------+\n",
      "|  20774718|        9|        Least Seller|\n",
      "|  20774718|        3|   Second top seller|\n",
      "|  20774718|        3|Only seller or mu...|\n",
      "+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_sales_union_df.where(col(\"product_id\") == 20774718).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea55b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cb823ab",
   "metadata": {},
   "source": [
    "## 4. Create a new column called \"hashed_bill\" defined as follows:\n",
    "- if the order_id is even: apply MD5 hashing iteratively to the bill_raw_text field, once for each 'A' (capital 'A') present in the text. E.g. if the bill text is 'nbAAnllA', you would apply hashing three times iteratively (only if the order number is even)\n",
    "- if the order_id is odd: apply SHA256 hashing to the bill text\n",
    "- Finally, check if there are any duplicate on the new column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c34f1",
   "metadata": {},
   "source": [
    "### 4.1. Define the UDF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo(order_id, bill_text):\n",
    "    #   If number is even\n",
    "    ret = bill_text.encode(\"utf-8\")\n",
    "    if int(order_id) % 2 == 0:\n",
    "        #   Count number of 'A'\n",
    "        cnt_A = bill_text.count(\"A\")\n",
    "        for _c in range(0, cnt_A):\n",
    "            ret = hashlib.md5(ret).hexdigest().encode(\"utf-8\")\n",
    "        ret = ret.decode(\"utf-8\")\n",
    "    else:\n",
    "        ret = hashlib.sha256(ret).hexdigest()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef8402",
   "metadata": {},
   "source": [
    "### 4.2. Register the UDF function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "363dff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_udf = spark.udf.register(\"algo\", algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ecdf70",
   "metadata": {},
   "source": [
    "### 4.3. Use the `algo_udf` to apply the aglorithm and then check if there is any duplicate hash in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922aaa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/19 15:20:01 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutivePartList.readAll(ParquetFileReader.java:1773)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:953)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:909)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1016)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:274)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:404)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:259)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1293)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:57)\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutivePartList.readAll(ParquetFileReader.java:1773)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:953)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:909)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1016)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:274)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:404)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:259)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1293)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:57)\n",
      "23/04/19 15:20:01 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutivePartList.readAll(ParquetFileReader.java:1777)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:953)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:909)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1016)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:274)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:404)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:259)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1293)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:57)\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader$ConsecutivePartList.readAll(ParquetFileReader.java:1777)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.internalReadRowGroup(ParquetFileReader.java:953)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:909)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readNextFilteredRowGroup(ParquetFileReader.java:1016)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase$ParquetRowGroupReaderImpl.readNextRowGroup(SpecificParquetRecordReaderBase.java:274)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:404)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:259)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1293)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/19 15:20:02 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/04/19 15:20:02 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:02 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:02 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:02 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:02 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:02 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:02 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:02 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:03 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:04 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:05 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:06 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:07 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:08 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/19 15:20:08 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:08 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:08 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/19 15:20:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/19 15:20:08 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:08 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/19 15:20:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/19 15:20:08 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/19 15:20:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/19 15:20:08 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:08 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:08 WARN TaskMemoryManager: Failed to allocate a page (2097136 bytes), try again.\n",
      "23/04/19 15:20:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/19 15:20:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/19 15:20:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/19 15:20:08 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/04/19 15:20:08 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/04/19 15:20:08 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/04/19 15:20:08 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/04/19 15:20:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 3:>                                                         (0 + 8) / 83]\r"
     ]
    }
   ],
   "source": [
    "sales_df.withColumn(\n",
    "    \"hashed_bill\", algo_udf(col(\"order_id\"), col(\"bill_raw_text\"))\n",
    ").groupby(col(\"hashed_bill\")).agg(count(\"*\").alias(\"cnt\")).where(col(\"cnt\") > 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee8ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
